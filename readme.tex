% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={readme},
  pdfauthor={Ethan Milne 74465159},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{readme}
\author{Ethan Milne 74465159}
\date{2025-03-04}

\begin{document}
\maketitle

\section{Resume Data Processing and NLP
Analysis}\label{resume-data-processing-and-nlp-analysis}

\subsection{Overview}\label{overview}

This project focuses on processing and analyzing resume data using NLP.
The analysis includes data cleaning, text preprocessing, topic modeling,
sentiment analysis and n-gram extraction.

\subsection{Features}\label{features}

\begin{itemize}
\tightlist
\item
  \textbf{Data Cleaning}: Removes unnecessary words, fills missing
  values, and processes text columns.
\item
  \textbf{Text Preprocessing}: Converts text to lowercase, removes
  punctuation, and filters stopwords.
\item
  \textbf{Bigram Extraction}: Identifies frequently occurring word pairs
  in resume descriptions.
\item
  \textbf{Topic Modeling (LDA)}: Extracts key topics from resume text
  data.
\item
  \textbf{Sentiment Analysis}: Analyzes sentiment polarity using the
  Bing lexicon.
\end{itemize}

\subsection{Installation}\label{installation}

\subsubsection{Dependencies}\label{dependencies}

Running this version of R. R version 4.4.0 (2024-04-24 ucrt) -- ``Puppy
Cup''

Ensure you have the following R packages installed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"broom"}\NormalTok{, }\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"tidytext"}\NormalTok{, }\StringTok{"dplyr"}\NormalTok{, }\StringTok{"re"}\NormalTok{, }\StringTok{"knitr"}\NormalTok{,}
                   \StringTok{"kableExtra"}\NormalTok{, }\StringTok{"RColorBrewer"}\NormalTok{, }\StringTok{"topicmodels"}\NormalTok{, }\StringTok{"tm"}\NormalTok{, }\StringTok{"stopwords"}\NormalTok{,}
                   \StringTok{"future"}\NormalTok{, }\StringTok{"future.apply"}\NormalTok{, }\StringTok{"Hmisc"}\NormalTok{, }\StringTok{"stringr"}\NormalTok{, }\StringTok{"rlang"}\NormalTok{, }\StringTok{"textdata"}\NormalTok{,}
                   \StringTok{"wordcloud"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{, }\StringTok{"patchwork"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection{Setup}\label{setup}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Pull the data set from Kaggle:
  \url{https://www.kaggle.com/datasets/saugataroyarghya/resume-dataset}
\item
  Clone the repository and set your working directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{current\_dir }\OtherTok{\textless{}{-}} \FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}
\item
  Load the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resume\_file }\OtherTok{\textless{}{-}} \FunctionTok{file.path}\NormalTok{(current\_dir, }\StringTok{\textquotesingle{}data/resume\_data.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{resume\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(resume\_file)}
\end{Highlighting}
\end{Shaded}
\item
  Load necessary scripts:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"00\_libraries.R"}\NormalTok{)}
\FunctionTok{source}\NormalTok{(}\StringTok{"01\_tools.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\subsection{Functions}\label{functions}

\subsubsection{\texorpdfstring{1.
\texttt{occurrences\_counter()}}{1. occurrences\_counter()}}\label{occurrences_counter}

Counts occurrences of unique values in a specified column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{occurrences\_counter}\NormalTok{(}\StringTok{"column\_name"}\NormalTok{, }\StringTok{"output\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{2.
\texttt{clean\_column()}}{2. clean\_column()}}\label{clean_column}

Removes specific words from a text column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{clean\_column}\NormalTok{(}\StringTok{"column\_name"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{3.
\texttt{preprocess\_text()}}{3. preprocess\_text()}}\label{preprocess_text}

Processes text by removing punctuation, converting to lowercase, and
filtering stopwords.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{preprocess\_text}\NormalTok{(}\StringTok{"sample text here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{4.
\texttt{generate\_bigrams()}}{4. generate\_bigrams()}}\label{generate_bigrams}

Extracts common n-grams from a text column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{generate\_bigrams}\NormalTok{(}\StringTok{"column\_name"}\NormalTok{, }\AttributeTok{score\_threshold =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{max\_n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{5.
\texttt{perform\_lda()}}{5. perform\_lda()}}\label{perform_lda}

Performs topic modeling using LDA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{perform\_lda}\NormalTok{(}\StringTok{"column\_name"}\NormalTok{, }\AttributeTok{num\_topics =} \DecValTok{3}\NormalTok{, }\AttributeTok{num\_words =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{6.
\texttt{perform\_sentiment\_analysis()}}{6. perform\_sentiment\_analysis()}}\label{perform_sentiment_analysis}

Analyzes sentiment distribution using the Bing lexicon.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{perform\_sentiment\_analysis}\NormalTok{(}\StringTok{"column\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Data Processing}\label{data-processing}

\subsubsection{Cleaning}\label{cleaning}

\begin{itemize}
\tightlist
\item
  Fills missing language fields with ``English''.
\item
  Removes unwanted words from specific text columns.
\end{itemize}

\subsubsection{Preprocessing}\label{preprocessing}

\begin{itemize}
\tightlist
\item
  Applies \texttt{preprocess\_text()} to clean text columns.
\item
  Uses \texttt{future\_lapply()} for efficient parallel processing.
\end{itemize}

\subsection{Output}\label{output}

\begin{itemize}
\tightlist
\item
  Cleaned resume dataset.
\item
  Visualizations for sentiment analysis and topic modeling.
\end{itemize}

\subsection{Future Enhancements}\label{future-enhancements}

\begin{itemize}
\tightlist
\item
  Improve recognition to classify skills and job titles and degree
  names. Either look for pattern or load dictionary to work against.
\item
  Research and then implement more robust text classification models.
  Identify what makes a text classification model robust.
\end{itemize}

\end{document}
